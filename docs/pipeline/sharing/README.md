# Sharing data

There are several ways to share data between the pipeline and users:
1. [Extract state vectors](#state-vector-extraction)
    1. python script
    1. python notebook
1. [Read](#read) manually managed data [from disk](#disk-access)
1. [Write](#write) intermediate data [to disk](#disk-access)

## State vector extraction

State vector extraction is the same if from a python script or from a python notebook. The information provided here is the same for either case despite it looking like a script but developed using a Jupyter Python Notebook.

### Sensitive information

We first have to define our sensitive information that grants us access to our pipeline. The information (content) will change depending upon the database you are using (postgresql or shelve) and, of course, your choices for usernames, passwords, etc. The best way to achieve this is to:
```
$ source <repostiory root>/.docker/.env
$ source <environment profile>
$ source /proj/sdp/ops/db-read-access  # only if desired
```

These two scripts should set all of the variables that you need to make your current environment look like the inside of a docker contaier.

If you cannot set the environment before starting your Python environment, most true with Jupyter Notebooks, then you will need to load them into os.environ within Python. First, you will need to install python-dotenv to avoid rewriting that which exists. For the `<environment profile>` it have to be processed manually. Here is an example that should work:
```
import os
def load_environment_profile(filename:str):
    with open(filename, 'rt') as file:
        for line in file.readlines():
            key,value = line.replace ('export ','').replace ('\\\n','').strip().split('=')
            os.environ[key] = os.path.expandvars(os.path.expanduser(value))
```

The function will also work for `<repository root>/.docker/.env` but it will overwrite any variable that is already there making python-dotenv better. Since it works in either a script or notebook, we will cover using the python mechanism:
```
# define the repository root that will be used later
repository_root = '/home/niessner/Projects/Exoplanet/esp'

# load the base environment variables that docker compose would normally set
load_environment_profile(os.path.join(repository_root, '.docker/.env'))

# override the base enviroment variables that define personal choices
load_environment_profile(os.path.join(repository_root, 'esp/envs/alsMT'))

# load the environment variables that give read access to the ops pipeline
# it should be used if the desire is to communicate with a private pipeline
load_environment_profile('/proj/sdp/ops/db-read-access')
```

### Connect to DB


Using our sensitive information, we connect to the database:
```
```

### Extract state vectors

In order to extract data, we need more information: Which run id? Which target? Which task, algorithm, and state vector? Which value of the state vector?

We now need to be more pedantic. The term "state vector" is used intentionally nebulous here. Like the word "name" it can have many meanings all at once. Technically, the state vector is a segment of the full name for any value generated by a pipeline. What in the heck is that supposed to mean? All pipelines generate software elements dawgie.Value that are stored and tracked by their full name: `run_id.target.task.algorith.state_vector.value`. Just like when we ask what is your name, name means many things depending on context. Informal it is your given name. Formal it is your surname. When we say state vector, we usually mean its full name to include all of its values. As in "extract a state vector" means we would need to know its full name not just the name given to the software object dawgie.StateVector. Hence, the need for all this new information.

State vector full name:
```
```

Now that we have the state vector's full name, we can ask for all its values:
```
```

If we only wanted one of the values in the state vector, we could have asked for just that as well:
```
```

## Disk access

### Read
### Write
