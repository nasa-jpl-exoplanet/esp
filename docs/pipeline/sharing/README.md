# Sharing data

There are several ways to share data between the pipeline and users:
1. [Extract state vectors](#state-vector-extraction)
    1. python script
    1. python notebook
1. [Read](#read) manually managed data [from disk](#disk-access)
1. [Write](#write) intermediate data [to disk](#disk-access)

## State vector extraction

State vector extraction is the same if from a python script or from a python notebook. The information provided here is the same for either case despite it looking like a script but developed using a Jupyter Python Notebook.

### Sensitive information

We first have to define our sensitive information that grants us access to our pipeline. The information (content) will change depending upon the database you are using (postgresql or shelve) and, of course, your choices for usernames, passwords, etc. The best way to achieve this is to:
```
$ source <repostiory root>/.docker/.env
$ source <environment profile>
$ source /proj/sdp/ops/db-read-access  # only if desired
```

These two scripts should set all of the variables that you need to make your current environment look like the inside of a docker contaier.

If you cannot set the environment before starting your Python environment, most true with Jupyter Notebooks, then you will need to load them into os.environ within Python. First, you will need to install python-dotenv to avoid rewriting that which exists. For the `<environment profile>` it have to be processed manually. Here is an example that should work:
```
import os
def load_environment_profile(filename:str):
    with open(filename, 'rt') as file:
        for line in file.readlines():
            key,value = line.replace ('export ','').replace ('\\\n','').strip().split('=')
            os.environ[key] = os.path.expandvars(os.path.expanduser(value))
```

The function will also work for `<repository root>/.docker/.env` but it will overwrite any variable that is already there making python-dotenv better. Since it works in either a script or notebook, we will cover using the python mechanism:
```
# define the repository root that will be used later
repository_root = '/home/niessner/Projects/Exoplanet/esp'

# load the base environment variables that docker compose would normally set
load_environment_profile(os.path.join(repository_root, '.docker/.env'))

# override the base enviroment variables that define personal choices
# in other words, change 'envs/alsMT' below to your <environment profile>
load_environment_profile(os.path.join(repository_root, 'esp/envs/alsMT'))

# load the environment variables that give read access to the ops pipeline
# it should be used if the desire is to communicate with a private pipeline
load_environment_profile('/proj/sdp/ops/db-read-access')
```

### Connect to DB

We have prepared the enivornment in the previous with out senstive information to configure dawgie as it starts. There is are two esp level configurations that we must also satisfy; one of which requires `repository_root` from the previous section:
```
# LDTK necessary bit of pain
import os; os.environ["LDTK_ROOT"] = '/proj/sdp/data/ldtk'

# add ESP software to the python path
import sys; sys.path.append(repository_root)
```

Using our sensitive information, we connect to the database. The first step is to import all of the required DAWGIE elements. The second step is to connect the security system to the desired pipeline. The third and last step is to open a connection to the database. However, since we are not full pipeline, rather a script requesting DB access, we should only do a `dawgie.db.reopen()`.

```
# initialize security and connect to the DB defined in through the environment"
import dawgie.db
import dawgie.context
import dawgie.security
dawgie.security.initialize(path=os.path.expandvars(
                    os.path.expanduser(dawgie.context.guest_public_keys)
                ),
                myname=dawgie.context.ssl_pem_myname,
                myself=os.path.expandvars(
                    os.path.expanduser(dawgie.context.ssl_pem_myself)
                ),
                system=dawgie.context.ssl_pem_file)
dawgie.db.reopen()
```

Ignore any warnings/errors about "No PGP keys found".

With the script/notebook connected to a DB via a running pipeline, we can now extract data directly from it.

### Extract state vectors

In order to extract data, we need more information: Which run id? Which target? Which task, algorithm, and state vector? Which value of the state vector?

We now need to be more pedantic. The term "state vector" is used intentionally nebulous here. Like the word "name" it can have many meanings all at once. Technically, the state vector is a segment of the full name for any value generated by a pipeline. What in the heck is that supposed to mean? All pipelines generate software elements dawgie.Value that are stored and tracked by their full name: `run_id.target.task.algorith.state_vector.value`. Just like when we ask what is your name, name means many things depending on context. Informal it is your given name. Formal it is your surname. When we say state vector, we usually mean its full name to include all of its values. As in "extract a state vector" means we would need to know its full name not just the name given to the software object dawgie.StateVector. Hence, the need for all this new information.

There are an infinite number of ways to obtain a state vector's full name. There are, however, two simple methods for obtaining state vector data: One, manually retrieve full name by [interactng](../interact#Database) from the running pipeline. Two, use an algorithm to load the data.

For both of these instances, we will want the latest or most recent data. To do this, we need a run id larger than any that exist. Therefore, we will use `runid = 2**30`. We will also need to specify a target, and, for documentation, we will use `target = 'GJ 1214'`. Lastly, we will declare the `task.alg.sv` name to be `system.finalize.parameters`.

#### by full Name

State vector full name:
```
```

If we only wanted one of the values in the state vector, we could have asked for just that as well:
```
```

#### by algoritm

#### extraction


```
```


## Disk access

### Read
### Write
